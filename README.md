### ‚öñÔ∏è The Efficiency of Explicit Width: 36 vs $2^n$

While the **Ensemble Hypothesis** suggests that a 9-layer Transformer creates $2^9$ (512) implicit paths, our research shows that **quantity is not quality**.

* **The Redundancy Tax**: Deep residual networks often suffer from path-overlap, where multiple layers compute redundant features.
* **The Sovereign Solution**: Our 36-cell manifold utilizes **Lateral Inhibition**. This forces the 36 experts to remain mathematically orthogonal (unique). 
* **The Result**: 36 "Strong Experts" outperform 512 "Weak Paths." We achieve competitive loss with a massive reduction in "ghost-path" computation, leading to our 3x speed advantage on CPU hardware.

## ‚öîÔ∏è Takeover Battle: Baseline Summary

**The Experiment:** A competition between **Hierarchical Depth** and **Parallel Resonance**.

### 1. STD (Standard Transformer)
* **Strategy**: **Deep Logic**.
* **Structure**: A 9-layer vertical stack of Transformer blocks.
* **Mechanism**: Relies on sequential Self-Attention to "reason" through text patterns.
* **Trade-off**: High computational cost on CPU due to layer depth ($O(L \cdot N^2)$).

### 2. GEO (Geometric Manifold)
* **Strategy**: **Broad Resonance**.
* **Structure**: A single-layer manifold with 6 parallel competitive cells.
* **Mechanism**: 
  * **Bio-Pulse**: Uses Sine-wave activations (`torch.sin`) to mimic neural firing.
  * **Inhibition**: Parallel cells compete via an inhibition matrix to reduce redundancy.
  * **Prototypes**: High-dimensional similarity routing instead of fixed sequential paths.
* **Trade-off**: Extremely fast; replaces serial depth with spatial width and rhythmic "rhythm" detection.

### üìä Structural Comparison

| Feature | STD (Standard) | GEO (Geometric) |
| :--- | :--- | :--- |
| **Philosophy** | Deep Sequential | Broad Parallel |
| **Logic** | Self-Attention | Competitive Resonance |
| **Activation** | GELU (Standard) | Sine-Pulse (Bio-inspired) |
| **CPU Speed** | Slower (Serial) | **Faster (Parallel)** |



**Goal:** Determine if a broad, competitive manifold can achieve a lower loss than a traditional deep stack by treating language as a resonant rhythm rather than a logical sequence.

[Results](https://github.com/MrPan2048/GeometricTransformer/blob/main/Baseline.md)


## ‚öîÔ∏è ByteFight

**The Experiment:** A head-to-head architectural battle between two "brains" processing the same raw byte stream.

### 1. STD (Standard Transformer)
* **Philosophy**: **Deep Logic**.
* **Mechanism**: Global Attention‚Äîevery byte looks at every other byte.
* **Performance**: Highly capable but computationally heavy. It suffers from **Quadratic Complexity ($O(N^2)$)**, leading to slower CPU speeds (150ms‚Äì240ms).

### 2. SGR (Sovereign)
* **Philosophy**: **Broad Manifold**.
* **Mechanism**: Local Convolution + Parallel Expert Cells. It mimics biological local connectivity.
* **Performance**: Highly efficient. It operates with **Linear Complexity ($O(N)$)**, running consistently **3x faster** (~55ms) than the standard model.

### üèÜ The Result
The **SGR (Sovereign)** model is currently winning on speed and hardware efficiency. It proves that for byte-level logic, a **wide, parallel spatial map** can outperform a **deep, sequential stack** while using significantly fewer computational resources.

[Results](https://github.com/MrPan2048/GeometricTransformer/blob/main/Bytefight.md)

# üß¨ Sovereign Geometric Routing (SGR): The "Living Cell" Alternative

### **The battle**

## 1. The Core Philosophy
Traditional AI architectures (Transformers) act as a **"Black Box"**‚Äîa brain with no inherent structure, relying on a massive soup of statistical signals to calculate global attention ($O(n^2)$). This is non-biological and computationally wasteful.

**SGR (Sovereign Geometric Routing)** proposes the **Living Cell** theory:
* **The Soma:** Every token embedding is a physical neuron body fixed in a high-dimensional territory.
* **The Pulse:** Each cell maintains an internal temporal memory (the "path").
* **Synaptic Recruitment:** Instead of firing every connection, the cell performs **Calculated Recruitment**. It only activates the specific synapses it needs to reach the next logical state.

[Results](https://github.com/MrPan2048/GeometricTransformer/blob/main/Livingcell.md)
