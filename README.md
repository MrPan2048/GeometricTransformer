## The SGR Manifold: Surpassing Transformer Efficiency via Singular Geometric Strikes

[Results](https://github.com/MrPan2048/GeometricTransformer/blob/main/Baseline.md)


## ‚öîÔ∏è ByteFight

**The Experiment:** A head-to-head architectural battle between two "brains" processing the same raw byte stream.

### 1. STD (Standard Transformer)
* **Philosophy**: **Deep Logic**.
* **Mechanism**: Global Attention‚Äîevery byte looks at every other byte.
* **Performance**: Highly capable but computationally heavy. It suffers from **Quadratic Complexity ($O(N^2)$)**, leading to slower CPU speeds (150ms‚Äì240ms).

### 2. SGR (Sovereign)
* **Philosophy**: **Broad Manifold**.
* **Mechanism**: Local Convolution + Parallel Expert Cells. It mimics biological local connectivity.
* **Performance**: Highly efficient. It operates with **Linear Complexity ($O(N)$)**, running consistently **3x faster** (~55ms) than the standard model.

### üèÜ The Result
The **SGR (Sovereign)** model is currently winning on speed and hardware efficiency. It proves that for byte-level logic, a **wide, parallel spatial map** can outperform a **deep, sequential stack** while using significantly fewer computational resources.

[Results](https://github.com/MrPan2048/GeometricTransformer/blob/main/bytefight.md)
